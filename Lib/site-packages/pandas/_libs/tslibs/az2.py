from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.chrome.options import Options
from bs4 import BeautifulSoup
import time
import pandas as pd
import random

# Define a static list of user agents
user_agents = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0.3 Safari/605.1.15",
    # Add more user agents as needed
]

# Setup Selenium WebDriver
def setup_driver(user_agent):
    options = Options()
    options.add_argument(f'user-agent={user_agent}')
    # For Chrome
    driver = webdriver.Chrome(executable_path=r'C:\chromedriver_win32\chromedriver.exe')

    # For Firefox, use webdriver.Firefox(executable_path='/path/to/geckodriver')
    return driver


def fetch_reviews(page):
    user_agent = random.choice(user_agents)
    driver = setup_driver(user_agent)
    url = "https://www.amazon.com/EveryDrop-Whirlpool-Refrigerator-Filter-Packaging/product-reviews/B00UB38V2A/"
    if page == 1:
        params = "ref=cm_cr_arp_d_viewopt_sr?ie=UTF8&reviewerType=all_reviews&sortBy=recent&pageNumber=1&filterByStar=critical"
    elif 2 <= page <= 9:
        params = f"ref=cm_cr_getr_d_paging_btm_next_{page}?ie=UTF8&reviewerType=all_reviews&sortBy=recent&pageNumber={page}&filterByStar=critical"
    else:
        params = "ref=cm_cr_arp_d_paging_btm_next_10?ie=UTF8&reviewerType=all_reviews&sortBy=recent&pageNumber=10&filterByStar=critical"
    
    full_url = f"{url}{params}"
    #headers = {'User-Agent': choice(user_agents)}
    driver.get(full_url)
    time.sleep(random.randint(2, 5))
    soup = BeautifulSoup(driver.page_source, 'html.parser')
    driver.quit()
    return soup

def extract_reviews(soup, reviews_list):
    reviews = soup.find_all('div', {'data-hook': 'review'})
    for review in reviews:
        title = review.find('a', {'data-hook': 'review-title'}).text.strip()
        rating = review.find('i', {'data-hook': 'review-star-rating'}).text.strip()
        body = review.find('span', {'data-hook': 'review-body'}).text.strip()
        date = review.find('span', {'data-hook': 'review-date'}).text.strip()
        name = review.find('span', {'class': 'a-profile-name'}).text.strip()
        
        reviews_list.append({
            "Name": name,
            "Date": date,
            "Title": title,
            "Rating": rating,
            "Body": body
        })

reviews_list = []

reviews_list = []
for page in range(1, 3):  # Example for 2 pages
    soup = fetch_reviews(page)
    extract_reviews(soup, reviews_list)
    time.sleep(random.randint(1, 5))  # Wait before fetching the next page

# Convert to DataFrame and save
df = pd.DataFrame(reviews_list)
df.to_excel('amazon_reviews_selenium.xlsx', index=False)
