
import requests
from bs4 import BeautifulSoup
from time import sleep
from random import randint
import pandas as pd
from fake_useragent import UserAgent
from itertools import cycle

# Initialize UserAgent
try:
    ua = UserAgent(use_cache_server=False)
except:
    # In case fetching the latest user agents fails, fall back to a predefined list
    ua = UserAgent(fallback='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3')

# Create a list of user_agents
user_agents = [ua.chrome, ua.google, ua['google chrome'], ua.firefox, ua.ff, ua.safari, ua.ie, ua.opera, ua.phantom]

# Create a cycle of user agents
user_agents_cycle = cycle(user_agents)


def fetch_reviews(page):
    user_agent = next(user_agents_cycle)
    # URL for page 1
    if page == 1:
        url = f"https://www.amazon.com/EveryDrop-Whirlpool-Refrigerator-Filter-Packaging/product-reviews/B00UB38V2A/ref=cm_cr_arp_d_viewopt_sr?ie=UTF8&reviewerType=all_reviews&sortBy=recent&pageNumber=1&filterByStar=critical"
    # URL for pages 2 to 9
    elif 2 <= page <= 9:
        url = f"https://www.amazon.com/EveryDrop-Whirlpool-Refrigerator-Filter-Packaging/product-reviews/B00UB38V2A/ref=cm_cr_getr_d_paging_btm_next_{page}?ie=UTF8&reviewerType=all_reviews&sortBy=recent&pageNumber={page}&filterByStar=critical"
    # URL for page 10
    else:
        url = "https://www.amazon.com/EveryDrop-Whirlpool-Refrigerator-Filter-Packaging/product-reviews/B00UB38V2A/ref=cm_cr_arp_d_paging_btm_next_10?ie=UTF8&reviewerType=all_reviews&sortBy=recent&pageNumber=10&filterByStar=critical"
    
    headers = {'User-Agent': user_agent}
    response = requests.get(url, headers=headers)
    if response.status_code == 200:
        return BeautifulSoup(response.content, 'html.parser')
    else:
        print(f"Failed to retrieve page {page}")
        return None

def extract_reviews(soup,reviews_list):
    reviews = soup.find_all('div', {'data-hook': 'review'})
    for review in reviews:
        title = review.find('a', {'data-hook': 'review-title'}).text.strip()
        rating = review.find('i', {'data-hook': 'review-star-rating'}).text.strip()
        body = review.find('span', {'data-hook': 'review-body'}).text.strip()
        # Extracting the review date
        date = review.find('span', {'data-hook': 'review-date'}).text.strip()
        
        # Extracting the reviewer's name
        name = review.find('span', {'class': 'a-profile-name'}).text.strip()
        
        reviews_list.append({
            "Name": name,
            "Date": date,
            "Title": title,
            "Rating": rating,
            "Body": body
        })
        # The location is typically part of the review date string in Amazon reviews.
        # If there's a specific location element, you would extract it similarly.
        print(f"Name: {name}\nDate: {date}\nTitle: {title}\nRating: {rating}\nBody: {body}\n{'-'*20}")

reviews_list = []  # Initialize an empty list to store review details

for page in range(1, 2):
    sleep(2)
    soup = fetch_reviews(page)
    if soup:
        extract_reviews(soup,reviews_list)
    if page < 10:  # No need to wait after the last page
        sleep_duration = randint(3, 8)
        print(f"Waiting for {sleep_duration} seconds before fetching the next page...")
        sleep(sleep_duration)

# Convert the list of dictionaries into a pandas DataFrame
df = pd.DataFrame(reviews_list)

# Export the DataFrame to an Excel file
df.to_excel('amazon1_reviews.xlsx', index=False)